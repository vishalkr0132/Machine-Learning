{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from pydub import AudioSegment\n",
    "from spleeter.separator import Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure FFmpeg is accessible\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"  # Adjust if needed\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load and preprocess audio: convert to mono and 16kHz WAV.\"\"\"\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "        processed_audio = \"temp_audio.wav\"\n",
    "        audio.export(processed_audio, format=\"wav\")\n",
    "        return processed_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_voices(audio_file):\n",
    "    \"\"\"Separate vocals and accompaniment using Spleeter.\"\"\"\n",
    "    try:\n",
    "        separator = Separator('spleeter:2stems')\n",
    "        output_dir = \"output_directory\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        separator.separate_to_file(audio_file, output_dir)\n",
    "        print(f\"Voices separated and saved in '{output_dir}'.\")\n",
    "        return os.path.join(output_dir, \"vocals.wav\"), os.path.join(output_dir, \"accompaniment.wav\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Spleeter: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio using Wav2Vec 2.0.\"\"\"\n",
    "    try:\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "        model = bundle.get_model()\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != bundle.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=bundle.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Transcription\n",
    "        with torch.inference_mode():\n",
    "            emissions, _ = model(waveform)\n",
    "            tokens = torch.argmax(emissions, dim=-1)\n",
    "            transcription = bundle.decode(tokens[0])\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_models\\2stems, running initialization to predict.\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Voices separated and saved in 'output_directory'.\n",
      "Error transcribing audio: Couldn't find appropriate backend to handle uri output_directory\\vocals.wav and format None.\n",
      "Error transcribing audio: Couldn't find appropriate backend to handle uri output_directory\\accompaniment.wav and format None.\n",
      "\n",
      "Transcription Results:\n",
      "Speaker 1 (Vocals): None\n",
      "Speaker 2 (Accompaniment): None\n"
     ]
    }
   ],
   "source": [
    "# Processing pipeline\n",
    "input_audio = \"segment_1.wav\"  # Replace with actual file path\n",
    "processed_audio = load_audio(input_audio)\n",
    "\n",
    "if processed_audio:\n",
    "    vocals, accompaniment = separate_voices(processed_audio)\n",
    "    if vocals and accompaniment:\n",
    "        speaker1_text = transcribe_audio(vocals)\n",
    "        speaker2_text = transcribe_audio(accompaniment)\n",
    "\n",
    "        print(\"\\nTranscription Results:\")\n",
    "        print(f\"Speaker 1 (Vocals): {speaker1_text}\")\n",
    "        print(f\"Speaker 2 (Accompaniment): {speaker2_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "# Ensure FFmpeg is accessible\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"  # Adjust if needed\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load and preprocess audio: convert to mono and 16kHz WAV.\"\"\"\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "        processed_audio = \"temp_audio.wav\"\n",
    "        audio.export(processed_audio, format=\"wav\")\n",
    "        return processed_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def separate_voices(audio_file):\n",
    "    \"\"\"Separate vocals and accompaniment using Spleeter.\"\"\"\n",
    "    try:\n",
    "        separator = Separator('spleeter:2stems')\n",
    "        output_dir = \"output_directory\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        separator.separate_to_file(audio_file, output_dir)\n",
    "\n",
    "        # Use absolute paths to ensure correct file locations\n",
    "        vocals_path = os.path.abspath(os.path.join(output_dir, \"vocals.wav\"))\n",
    "        accompaniment_path = os.path.abspath(os.path.join(output_dir, \"accompaniment.wav\"))\n",
    "\n",
    "        print(f\"Voices separated and saved in '{output_dir}'.\")\n",
    "        return vocals_path, accompaniment_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Spleeter: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio using Wav2Vec 2.0, using librosa for compatibility.\"\"\"\n",
    "    try:\n",
    "        # Load Wav2Vec 2.0 ASR model\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "        model = bundle.get_model()\n",
    "\n",
    "        # Load audio using librosa (instead of torchaudio)\n",
    "        waveform, sample_rate = librosa.load(audio_file, sr=bundle.sample_rate)\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "        # Transcription\n",
    "        with torch.inference_mode():\n",
    "            emissions, _ = model(waveform)\n",
    "            tokens = torch.argmax(emissions, dim=-1)\n",
    "            transcription = bundle.decode(tokens[0])\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio {audio_file}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_models\\2stems, running initialization to predict.\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Voices separated and saved in 'output_directory'.\n",
      "\n",
      "Starting transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonam Kumari\\AppData\\Local\\Temp\\ipykernel_17376\\611242878.py:50: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, sample_rate = librosa.load(audio_file, sr=bundle.sample_rate)\n",
      "C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error transcribing audio d:\\Data Analysis\\AliceBlue\\output_directory\\vocals.wav: [Errno 2] No such file or directory: 'd:\\\\Data Analysis\\\\AliceBlue\\\\output_directory\\\\vocals.wav'\n",
      "Error transcribing audio d:\\Data Analysis\\AliceBlue\\output_directory\\accompaniment.wav: [Errno 2] No such file or directory: 'd:\\\\Data Analysis\\\\AliceBlue\\\\output_directory\\\\accompaniment.wav'\n",
      "\n",
      "Transcription Results:\n",
      "Speaker 1 (Vocals): None\n",
      "Speaker 2 (Accompaniment): None\n"
     ]
    }
   ],
   "source": [
    "# Processing pipeline\n",
    "input_audio = \"segment_1.wav\"  # Replace with actual file path\n",
    "processed_audio = load_audio(input_audio)\n",
    "\n",
    "if processed_audio:\n",
    "    vocals, accompaniment = separate_voices(processed_audio)\n",
    "    if vocals and accompaniment:\n",
    "        print(\"\\nStarting transcription...\")\n",
    "        speaker1_text = transcribe_audio(vocals)\n",
    "        speaker2_text = transcribe_audio(accompaniment)\n",
    "\n",
    "        print(\"\\nTranscription Results:\")\n",
    "        print(f\"Speaker 1 (Vocals): {speaker1_text}\")\n",
    "        print(f\"Speaker 2 (Accompaniment): {speaker2_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "# Ensure FFmpeg is accessible\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"  # Adjust if needed\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load and preprocess audio: convert to mono and 16kHz WAV.\"\"\"\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "        processed_audio = \"temp_audio.wav\"\n",
    "        audio.export(processed_audio, format=\"wav\")\n",
    "        return processed_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def separate_voices(audio_file):\n",
    "    \"\"\"Separate vocals and accompaniment using Spleeter.\"\"\"\n",
    "    try:\n",
    "        separator = Separator('spleeter:2stems')\n",
    "        output_dir = \"output_directory\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        separator.separate_to_file(audio_file, output_dir)\n",
    "\n",
    "        # Check actual files created by Spleeter\n",
    "        subdirs = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
    "        if subdirs:\n",
    "            output_dir = os.path.join(output_dir, subdirs[0])  # Update path to subdirectory\n",
    "\n",
    "        vocals_path = os.path.abspath(os.path.join(output_dir, \"vocals.wav\"))\n",
    "        accompaniment_path = os.path.abspath(os.path.join(output_dir, \"accompaniment.wav\"))\n",
    "\n",
    "        print(f\"\\n✅ Voices separated successfully. Check: {output_dir}\")\n",
    "        print(f\"🔹 Vocals Path: {vocals_path}\")\n",
    "        print(f\"🔹 Accompaniment Path: {accompaniment_path}\")\n",
    "\n",
    "        # Verify files exist\n",
    "        if not os.path.exists(vocals_path) or not os.path.exists(accompaniment_path):\n",
    "            print(\"❌ Error: Separated audio files were not found. Check Spleeter output.\")\n",
    "            return None, None\n",
    "\n",
    "        return vocals_path, accompaniment_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Spleeter: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio using Wav2Vec 2.0, using librosa for compatibility.\"\"\"\n",
    "    try:\n",
    "        # Load Wav2Vec 2.0 ASR model\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "        model = bundle.get_model()\n",
    "\n",
    "        # Load audio using librosa\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"❌ Error: File not found {audio_file}\")\n",
    "            return None\n",
    "\n",
    "        waveform, sample_rate = librosa.load(audio_file, sr=bundle.sample_rate)\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "        # Transcription\n",
    "        with torch.inference_mode():\n",
    "            emissions, _ = model(waveform)\n",
    "            tokens = torch.argmax(emissions, dim=-1)\n",
    "            transcription = bundle.decode(tokens[0])\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio {audio_file}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models\\2stems\\model\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Error in Spleeter: FFMPEG error: b\"ffmpeg version 7.1-full_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers\\r\\n  built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\\r\\n  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libaribb24 --enable-libaribcaption --enable-libdav1d --enable-libdavs2 --enable-libopenjpeg --enable-libquirc --enable-libuavs3d --enable-libxevd --enable-libzvbi --enable-libqrencode --enable-librav1e --enable-libsvtav1 --enable-libvvenc --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs2 --enable-libxeve --enable-libxvid --enable-libaom --enable-libjxl --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-liblensfun --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libcodec2 --enable-libilbc --enable-libgsm --enable-liblc3 --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint\\r\\n  libavutil      59. 39.100 / 59. 39.100\\r\\n  libavcodec     61. 19.100 / 61. 19.100\\r\\n  libavformat    61.  7.100 / 61.  7.100\\r\\n  libavdevice    61.  3.100 / 61.  3.100\\r\\n  libavfilter    10.  4.100 / 10.  4.100\\r\\n  libswscale      8.  3.100 /  8.  3.100\\r\\n  libswresample   5.  3.100 /  5.  3.100\\r\\n  libpostproc    58.  3.100 / 58.  3.100\\r\\n[aist#0:0/pcm_f32le @ 00000199b1638000] Guessed Channel Layout: stereo\\r\\nInput #0, f32le, from 'pipe:':\\r\\n  Duration: N/A, bitrate: 2822 kb/s\\r\\n  Stream #0:0: Audio: pcm_f32le, 44100 Hz, stereo, flt, 2822 kb/s\\r\\n[out#0/wav @ 00000199b163a200] Error opening output output_directory\\\\temp_audio/vocals.wav: Permission denied\\r\\nError opening output file output_directory\\\\temp_audio/vocals.wav.\\r\\nError opening output files: Permission denied\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "# Processing pipeline\n",
    "input_audio = \"segment_1.wav\"  # Replace with actual file path\n",
    "processed_audio = load_audio(input_audio)\n",
    "\n",
    "if processed_audio:\n",
    "    vocals, accompaniment = separate_voices(processed_audio)\n",
    "    if vocals and accompaniment:\n",
    "        print(\"\\nStarting transcription...\")\n",
    "        speaker1_text = transcribe_audio(vocals)\n",
    "        speaker2_text = transcribe_audio(accompaniment)\n",
    "\n",
    "        print(\"\\nTranscription Results:\")\n",
    "        print(f\"Speaker 1 (Vocals): {speaker1_text}\")\n",
    "        print(f\"Speaker 2 (Accompaniment): {speaker2_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "# Ensure FFmpeg is accessible\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"  # Adjust if needed\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load and preprocess audio: convert to mono and 16kHz WAV.\"\"\"\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "        processed_audio = \"temp_audio.wav\"\n",
    "        audio.export(processed_audio, format=\"wav\")\n",
    "        return processed_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def separate_voices(audio_file):\n",
    "    \"\"\"Separate vocals and accompaniment using Spleeter.\"\"\"\n",
    "    try:\n",
    "        separator = Separator('spleeter:2stems')\n",
    "        output_dir = \"D:/Data Analysis/AliceBlue/output_directory/temp_audio\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Run Spleeter without creating subdirectories\n",
    "        separator.separate_to_file(audio_file, output_dir)\n",
    "\n",
    "        # Spleeter saves files as 'vocals.wav' and 'accompaniment.wav'\n",
    "        vocals_path = os.path.join(output_dir, \"vocals.wav\")\n",
    "        accompaniment_path = os.path.join(output_dir, \"accompaniment.wav\")\n",
    "\n",
    "        # Check if files exist\n",
    "        if os.path.exists(vocals_path) and os.path.exists(accompaniment_path):\n",
    "            print(f\"\\n✅ Voices separated successfully. Check: {output_dir}\")\n",
    "            print(f\"🔹 Vocals Path: {vocals_path}\")\n",
    "            print(f\"🔹 Accompaniment Path: {accompaniment_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Error: Separated audio files not found or named differently in {output_dir}\")\n",
    "\n",
    "        return vocals_path, accompaniment_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Spleeter: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio using Wav2Vec 2.0, using librosa for compatibility.\"\"\"\n",
    "    try:\n",
    "        # Load Wav2Vec 2.0 ASR model\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "        model = bundle.get_model()\n",
    "\n",
    "        # Load audio using librosa\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"❌ Error: File not found {audio_file}\")\n",
    "            return None\n",
    "\n",
    "        waveform, sample_rate = librosa.load(audio_file, sr=bundle.sample_rate)\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "        # Transcription\n",
    "        with torch.inference_mode():\n",
    "            emissions, _ = model(waveform)\n",
    "            tokens = torch.argmax(emissions, dim=-1)\n",
    "            transcription = bundle.decode(tokens[0])\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio {audio_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models\\2stems\\model\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Processing pipeline\n",
    "input_audio = \"REC30AirportTravel.wav\"  # Replace with actual file path\n",
    "processed_audio = load_audio(input_audio)\n",
    "\n",
    "if processed_audio:\n",
    "    vocals, accompaniment = separate_voices(processed_audio)\n",
    "    if vocals and accompaniment:\n",
    "        print(\"\\nStarting transcription...\")\n",
    "        speaker1_text = transcribe_audio(vocals)\n",
    "        speaker2_text = transcribe_audio(accompaniment)\n",
    "\n",
    "        print(\"\\nTranscription Results:\")\n",
    "        print(f\"Speaker 1 (Vocals): {speaker1_text}\")\n",
    "        print(f\"Speaker 2 (Accompaniment): {speaker2_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
