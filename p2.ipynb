{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def load_audio(file_path):\n",
    "    # Load audio file\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    # Convert to mono and set sample rate to 16kHz (if needed)\n",
    "    audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "    # Export as WAV\n",
    "    audio.export(\"temp_audio.wav\", format=\"wav\")\n",
    "    return \"temp_audio.wav\"\n",
    "\n",
    "audio_file = load_audio(\"REC30AirportTravel.wav\")  # Replace with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_models\\2stems, running initialization to predict.\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "WARNING:tensorflow:From C:\\Users\\Sonam Kumari\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Voices separated and saved in 'output_directory'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set FFmpeg path manually\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"  # Adjust this path if needed\n",
    "\n",
    "# Now, run your Spleeter script\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "def separate_voices(audio_file):\n",
    "    try:\n",
    "        separator = Separator('spleeter:2stems')\n",
    "        separator.separate_to_file(audio_file, \"output_directory\")\n",
    "        print(\"Voices separated and saved in 'output_directory'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "audio_file = 'REC30AirportTravel.wav'\n",
    "separate_voices(audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: ffmpeg binary not found\n"
     ]
    }
   ],
   "source": [
    "# from spleeter.separator import Separator\n",
    "\n",
    "# def separate_voices(audio_file):\n",
    "#     try:\n",
    "#         # Initialize Spleeter separator (2 stems: vocals and accompaniment)\n",
    "#         separator = Separator('spleeter:2stems')\n",
    "        \n",
    "#         # Separate voices\n",
    "#         separator.separate_to_file(audio_file, \"output_directory\")\n",
    "        \n",
    "#         print(\"Voices separated and saved in 'output_directory'.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "# # Replace 'your_audio_file.mp3' with the path to your audio file\n",
    "# audio_file = 'REC30AirportTravel.wav'\n",
    "# separate_voices(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    # Load pre-trained Wav2Vec 2.0 model\n",
    "    bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "    model = bundle.get_model()\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=bundle.sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Transcribe\n",
    "    with torch.inference_mode():\n",
    "        emissions, _ = model(waveform)\n",
    "        tokens = torch.argmax(emissions, dim=-1)\n",
    "        transcription = bundle.decode(tokens[0])\n",
    "\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_models\\2stems, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Voices separated and saved in 'output_directory'.\n",
      "Error transcribing audio: Couldn't find appropriate backend to handle uri output_directory\\vocals.wav and format None.\n",
      "Error transcribing audio: Couldn't find appropriate backend to handle uri output_directory\\accompaniment.wav and format None.\n",
      "\n",
      "Transcription Results:\n",
      "Speaker 1 (Vocals): None\n",
      "Speaker 2 (Accompaniment): None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from pydub import AudioSegment\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "# Ensure FFmpeg is accessible\n",
    "os.environ[\"PATH\"] += os.pathsep + \"C:\\\\ffmpeg\\\\bin\"  # Adjust if needed\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load and preprocess audio: convert to mono and 16kHz WAV.\"\"\"\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "        processed_audio = \"temp_audio.wav\"\n",
    "        audio.export(processed_audio, format=\"wav\")\n",
    "        return processed_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def separate_voices(audio_file):\n",
    "    \"\"\"Separate vocals and accompaniment using Spleeter.\"\"\"\n",
    "    try:\n",
    "        separator = Separator('spleeter:2stems')\n",
    "        output_dir = \"output_directory\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        separator.separate_to_file(audio_file, output_dir)\n",
    "        print(f\"Voices separated and saved in '{output_dir}'.\")\n",
    "        return os.path.join(output_dir, \"vocals.wav\"), os.path.join(output_dir, \"accompaniment.wav\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Spleeter: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio using Wav2Vec 2.0.\"\"\"\n",
    "    try:\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "        model = bundle.get_model()\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != bundle.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=bundle.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Transcription\n",
    "        with torch.inference_mode():\n",
    "            emissions, _ = model(waveform)\n",
    "            tokens = torch.argmax(emissions, dim=-1)\n",
    "            transcription = bundle.decode(tokens[0])\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "# Processing pipeline\n",
    "input_audio = \"segment_1.wav\"  # Replace with actual file path\n",
    "processed_audio = load_audio(input_audio)\n",
    "\n",
    "if processed_audio:\n",
    "    vocals, accompaniment = separate_voices(processed_audio)\n",
    "    if vocals and accompaniment:\n",
    "        speaker1_text = transcribe_audio(vocals)\n",
    "        speaker2_text = transcribe_audio(accompaniment)\n",
    "\n",
    "        print(\"\\nTranscription Results:\")\n",
    "        print(f\"Speaker 1 (Vocals): {speaker1_text}\")\n",
    "        print(f\"Speaker 2 (Accompaniment): {speaker2_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m             conversation\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeaker 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker2_sentences[i]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(conversation)\n\u001b[1;32m---> 14\u001b[0m formatted_conversation \u001b[38;5;241m=\u001b[39m \u001b[43mformat_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker1_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker2_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(formatted_conversation)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mformat_conversation\u001b[1;34m(speaker1_text, speaker2_text)\u001b[0m\n\u001b[0;32m      2\u001b[0m conversation \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Split sentences (assuming sentences are separated by periods)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m speaker1_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker1_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m speaker2_sentences \u001b[38;5;241m=\u001b[39m speaker2_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Format conversation\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "def format_conversation(speaker1_text, speaker2_text):\n",
    "    conversation = []\n",
    "    # Split sentences (assuming sentences are separated by periods)\n",
    "    speaker1_sentences = speaker1_text.split('. ')\n",
    "    speaker2_sentences = speaker2_text.split('. ')\n",
    "    # Format conversation\n",
    "    for i in range(max(len(speaker1_sentences), len(speaker2_sentences))):\n",
    "        if i < len(speaker1_sentences):\n",
    "            conversation.append(f\"Speaker 1: {speaker1_sentences[i].strip()}\")\n",
    "        if i < len(speaker2_sentences):\n",
    "            conversation.append(f\"Speaker 2: {speaker2_sentences[i].strip()}\")\n",
    "    return \"\\n\".join(conversation)\n",
    "\n",
    "formatted_conversation = format_conversation(speaker1_text, speaker2_text)\n",
    "print(formatted_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
